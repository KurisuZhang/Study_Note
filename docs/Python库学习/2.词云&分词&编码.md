# 词云&分词&编码

[词云(word cloud)](https://github.com/amueller/word_cloud)  
[结巴(jieba)](https://github.com/fxsjy/jieba)

- 词云默认不支持中文
- 需要将支持中文的字体添加到site-packages\wordcloud\wordcloud.py,并修改
    - FONT_PATH = os.environ.get('FONT_PATH', os.path.join(FILE, 'msyh.ttc'))

```python
# 画图
from matplotlib import pyplot as plt
# 制作词云
from wordcloud import WordCloud
# 制作mask
from PIL import Image
import numpy as np
# 分词
import jieba
```

## 文本的处理与编码

```python
# 去除换行
def strip_the_line(path):
    with open(path,'r') as f:
        for line in f.readlines():
            a=a+line.strip('\n')
    return a

# 第二种方法
#使用正则里的\s
```

```python
# 去除标点符号
def strip_the_puntuation():
    text = re.sub("[\s！？｡。?＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.]", "",text)
```


```python
# 获取和更改文本编码
import chardet
import codecs
# 获取编码
with open("1.txt", "rb") as f:
    buf = f.read()
    result = chardet.detect(buf)
    print(result['encoding'])

# 更改编码,这里填写获取到的编码
with open("1.txt", 'r', encoding=result['encoding']) as f1:  # 填写获取的编码
    new_content = f1.read()
    with codecs.open("2.txt", 'w', encoding="GB2312") as f2:  # 填写要转换的编码
        f2.write(new_content)
```

## jieba分词


```python
import jieba

seg_list = jieba.cut("我来到北京清华大学", cut_all=True)
print("全模式: " + "/ ".join(seg_list))  # 全模式
'''
全模式: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学
'''

seg_list = jieba.cut("我来到北京清华大学", cut_all=False)
print("精准模式: " + "/ ".join(seg_list))  # 精确模式
'''
精准模式: 我/ 来到/ 北京/ 清华大学
'''

seg_list = jieba.cut("他来到了网易杭研大厦")  # 默认是精确模式
print("/ ".join(seg_list))
'''
他/ 来到/ 了/ 网易/ 杭研/ 大厦
'''

seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式
print("/".join(seg_list))
'''
小明/硕士/毕业/于/中国/科学/学院/科学院/中国科学院/计算/计算所/，/后/在/日本/京都/大学/日本京都大学/深造
'''
```

    Building prefix dict from the default dictionary ...
    Loading model from cache C:\Users\linch\AppData\Local\Temp\jieba.cache
    Loading model cost 1.024 seconds.
    Prefix dict has been built succesfully.


    全模式: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学
    精准模式: 我/ 来到/ 北京/ 清华大学
    他/ 来到/ 了/ 网易/ 杭研/ 大厦
    小明/硕士/毕业/于/中国/科学/学院/科学院/中国科学院/计算/计算所/，/后/在/日本/京都/大学/日本京都大学/深造


    '\n小明/硕士/毕业/于/中国/科学/学院/科学院/中国科学院/计算/计算所/，/后/在/日本/京都/大学/日本京都大学/深造\n'



## wordcloud词云

```python
import jieba
from wordcloud import WordCloud
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import re

# 打开文件
with open("1.txt","r",encoding="utf-8") as f:
    text = f.read()

# text = re.sub("[\s！？｡。?＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.]", "",text)

# 使用分词
seg_list = jieba.cut(text)
text = " ".join(seg_list)  # 全模式
print(text)

# 词云相关操作
img = Image.open(r'mask.png') #打开图片
img_array = np.array(img) #将图片装换为数组，制作mask
# #设置停止词，也就是你不想显示的词，这里这个词是我前期处理没处理好，你可以删掉他看看他的作用
stopword=[]
wc = WordCloud(
    background_color='white',
    width=1000,
    height=800,
    mask=img_array,
    stopwords=stopword).generate(text)

# plt.imshow(wc)
# plt.axis("off")
# plt.show()
wc.to_file("w.png")
```
